import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Conv2D,Conv1D,MaxPooling1D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.regularizers import l2 

data = pd.read_csv("dataset-malware3.csv")

#data['classification'] = data.classification.map({'B': 0, 'S': 1})
#data = data.sample(frac=1).reset_index(drop=True)

#sns.countplot(data["classification"])
data2 = pd.read_csv("dataset-malware2.csv")

data2['classification'] = data2.classification.map({'B': 0, 'S': 1})
data2 = data2.sample(frac=1).reset_index(drop=True)

sns.countplot(data2["classification"])
#plt.show()

# Assuming each row in the dataset represents a PE header
# Adjust the input size according to the size of your PE headers
input_size = 81  # Update with the appropriate size
output_size = 2

# Assuming you have 1D feature vectors for each PE header
#X = np.array(data.drop(["Name", "classification"], axis=1))
X=data.values

# Reshape the data for 2D convolution
X = X.reshape(-1, input_size, 1, 1)

print(X.shape)

Y = data2["classification"]
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)

# Standardize the input data
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train.reshape(-1, input_size)).reshape(-1, input_size, 1, 1)
x_test = scaler.transform(x_test.reshape(-1, input_size)).reshape(-1, input_size, 1, 1)



model = tf.keras.Sequential([
    Conv2D(256, kernel_size=(3, 1), activation='relu',kernel_regularizer=l2(0.01), input_shape=(input_size, 1, 1)),
    MaxPooling2D(pool_size=(2, 1)),  # Adjust pool_size
    Conv2D(128, kernel_size=(3, 1), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 1)),  # Adjust pool_size
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(output_size, activation='softmax')
])

def lr_schedule(epoch):
    return 0.001 * 0.9**epoch

#optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)
optimizer = Adam(learning_rate=0.001)

model.summary()
model.compile(optimizer= optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

batch_size = 10000
max_epochs = 20

early_stopping = EarlyStopping(patience=5, restore_best_weights=True)

result = model.fit(x=x_train,
                   y=y_train,
                   batch_size=batch_size,
                   epochs=max_epochs,
                   verbose=1,
                   callbacks=[early_stopping],
                   validation_split=0.2)

# Visualize the result
acc = result.history['accuracy']
val_acc = result.history['val_accuracy']
loss = result.history['loss']
val_loss = result.history['val_loss']

epochs = range(1, len(acc) + 1)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
sns.set_style("white")
plt.suptitle('Training Results', size=15)

ax1.plot(epochs, acc, "bo", label="Training acc")
ax1.plot(epochs, val_acc, "b", label="Validation acc")
ax1.set_title("Training and validation acc")
ax1.legend()
ax1.set_ylabel('Accuracy')  # Set y-axis label
ax1.set_xlabel('Epoch')

ax2.plot(epochs, loss, "bo", label="Training loss", color='red')
ax2.plot(epochs, val_loss, "b", label="Validation loss", color='red')
ax2.set_title("Training and validation loss")
ax2.legend()
ax2.set_ylabel('Cost')  # Set y-axis label
ax2.set_xlabel('Epoch')

plt.show()

test_loss, test_accuracy = model.evaluate(x_test, y_test)
print('\nTest loss: {0:.6f}. Test accuracy: {1:.6f}%'.format(test_loss, test_accuracy * 100.))
