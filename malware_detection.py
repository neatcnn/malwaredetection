import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.initializers import glorot_uniform

data =pd.read_csv(r"dataset-malware.csv")

data.describe(include="all")
data['classification'] =data.classification.map({'B':0, 'S':1}) # the values of the y column should be converted to 0 and 1
#1 for malware and 0 for benign 
data["classification"].value_counts()
data = data.sample(frac=1).reset_index(drop=True)
sns.countplot(data["classification"])

#X = data.drop(["hash","classification",'vm_truncate_count','shared_vm','exec_vm','nvcsw','maj_flt','utime'],axis=1)
#Y = data["classification"]

X = data.drop(["Name","classification"],axis=1)
Y = data["classification"]
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=1)

scaler = StandardScaler()

x_train = scaler.fit_transform(x_train)
x_test = scaler.fit_transform(x_test)

input_size = 77 
#input_size = 215

#Number of Outputs
output_size = 2 

# Use same hidden layer size for both hidden layers. Not a necessity.
hidden_layer_size = 4
    
# define how the model will look like
model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_layer_size, input_shape=(input_size,), activation='tanh', kernel_initializer=glorot_uniform()), # activation ftn: 2. relu , 3. tanh , 1. tanh
    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),
    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),
    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),
    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(output_size, activation='softmax') # output layer - 1. softmax1 , 2. sigmoid
])

model.summary()

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# optimizers 2. adam, 3. sgd , 1. RMSprop, Adagrad
# loss function - 1. sparse_categorical_crossentropy, 2. binary_crossentropy (not working)

# set the batch size
batch_size = 10000

# set a maximum number of training epochs
max_epochs = 300

# set an early stopping mechanism
# let's set patience=2, to be a bit tolerant against random validation loss increases
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)
# def lr_schedule(epoch, lr):
#     # Custom learning rate schedule
#     return lr * 0.1 if epoch >= 50 else lr

# lr_callback = LearningRateScheduler(lr_schedule)

result = model.fit(x=x_train,
                   y=y_train,
                   batch_size=5000,
                   epochs=max_epochs,
                   verbose=1,
                   initial_epoch=50, 
                   #callbacks=[early_stopping, lr_callback],
                   callbacks=[early_stopping],
                   validation_split=0.2)

# smaller batch sizes might provide a more stochastic training process, 
# while larger sizes could lead to faster convergence but require more memory
# Adjust the number of training epochs. 
# Training for more epochs could lead to overfitting, while too few epochs might result in underfitting.
# Implement learning rate scheduling to dynamically adjust the learning rate during training. This can help improve convergence.

# Visualize the result
acc = result.history['accuracy']
val_acc = result.history['val_accuracy']
loss = result.history['loss']
val_loss = result.history['val_loss']

epochs = range(1, len(acc) + 1)


#fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
#sns.set_style("white")
#plt.suptitle('Training Results', size = 15)


# ax1.plot(epochs, acc, "bo", label = "Training accuracy")
# ax1.plot(epochs, val_acc, "b", label = "Validation accuracy")
# ax1.set_title("Training and validation acc")
# ax1.legend()
# ax1.set_ylabel('Accuracy')  # Set y-axis label
# ax1.set_xlabel('Epoch') 

# ax2.plot(epochs, loss, "bo", label = "Training loss", color = 'red')
# ax2.plot(epochs, val_loss, "b", label = "Validation loss", color = 'red')
# ax2.set_title("Training and validation loss")
# ax2.legend()
# ax2.set_ylabel('Cost')  # Set y-axis label
# ax2.set_xlabel('Epoch') 

plt.figure(figsize=(10, 5))
sns.set_style("white")

# Plot accuracy
plt.plot(epochs, acc, "bo", label="Training accuracy")
plt.plot(epochs, val_acc, "b", label="Validation accuracy")

# Plot loss on the same plot
#plt.plot(epochs, loss, "ro", label="Training loss")
#plt.plot(epochs, val_loss, "r", label="Validation loss")

#plt.title("Training Results")

plt.ylim(0, 1)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Accuracy')


plt.show()
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print('\nTest loss: {0:.6f}. Test accuracy: {1:.6f}'.format(test_loss, test_accuracy))









