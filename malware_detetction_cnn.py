import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

data = pd.read_csv("dataset-malware.csv")

# Map 'B' to 0 and 'S' to 1
data['classification'] = data['classification'].map({'B': 0, 'S': 1})

# Shuffle the dataset
data = data.sample(frac=1).reset_index(drop=True)

sns.countplot(data["classification"])
#plt.show()

# Split into features (X) and labels (Y)
X = data.drop(["Name", "classification"], axis=1)
Y = data["classification"]

# Determine the number of features in the dataset
num_samples, num_features = X.shape

# Reshape X into a 2D array
X_reshaped = X.values.reshape((num_samples, num_features))

# Split into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X_reshaped, Y, test_size=0.2, random_state=1)

# Reshape the input data for compatibility with Conv1D
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

# Standardize the data
scaler = StandardScaler()

# Standardize training data
x_train = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1)).reshape(x_train.shape)
# Standardize testing data
x_test = scaler.transform(x_test.reshape(x_test.shape[0], -1)).reshape(x_test.shape)


# Model parameters
output_size = 2

# Define the model with Convolutional layers
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(78,1, 1)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),  # Adjust padding if needed
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(output_size, activation='softmax')
])

model.summary()

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
batch_size = 1000
max_epochs = 30
early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)

result = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=max_epochs, verbose=1,
                   validation_split=0.4)

# Visualize the result
plt.plot(result.history['accuracy'], label='train')
plt.legend(loc='upper left')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.show()

plt.plot(result.history['loss'], label='train')
plt.legend(loc='upper right')
plt.title('Model Cost')
plt.ylabel('Cost')
plt.xlabel('Epoch')
plt.show()

# Evaluate on test set
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print('\nTest loss: {0:.6f}. Test accuracy: {1:.6f}%'.format(test_loss, test_accuracy * 100.))

# Additional training with early stopping
result = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=100, verbose=1,
                   initial_epoch=20, callbacks=[early_stopping], validation_split=0.2)

# Visualize the result
plt.plot(result.history['accuracy'], label='train')
plt.legend(loc='upper left')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.show()

plt.plot(result.history['loss'], label='train')
plt.legend(loc='upper right')
plt.title('Model Cost')
plt.ylabel('Cost')
plt.xlabel('Epoch')
plt.show()

# Evaluate on test set after additional training
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print('\nTest loss: {0:.6f}. Test accuracy: {1:.6f}%'.format(test_loss, test_accuracy * 100.))