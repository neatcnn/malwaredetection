import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.regularizers import l2

data = pd.read_csv("dataset-malware3.csv")

data2 = pd.read_csv("dataset-malware2.csv")
data2['classification'] = data2.classification.map({'B': 0, 'S': 1})
data2 = data2.sample(frac=1).reset_index(drop=True)
sns.countplot(data2["classification"])

# Assuming each row in the dataset represents a PE header
# Adjust the input size according to the size of your PE headers
input_size = 81  # Update with the appropriate size
output_size = 2

# Assuming you have 1D feature vectors for each PE header
X = data.values
Y = data2["classification"]
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)

# Standardize the input data
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Reshape the data for 2D convolution
x_train = x_train.reshape(-1, input_size, 1, 1)
x_test = x_test.reshape(-1, input_size, 1, 1)

# Create ImageDataGenerator
#datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1)
datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    shear_range=0.1,
    horizontal_flip=True,
    vertical_flip=True
)

# Build a CNN model
# model = tf.keras.Sequential([
#     Conv2D(128, kernel_size=(3, 1), activation='relu', kernel_regularizer=l2(0.02), input_shape=(input_size, 1, 1)),
#     MaxPooling2D(pool_size=(2, 1)),
#     Conv2D(128, kernel_size=(3, 1), activation='relu', padding='same'),
#     MaxPooling2D(pool_size=(2, 1)),
#     Flatten(),
#     Dense(512, activation='relu'),
#     Dropout(0.2),
#     Dense(output_size, activation='softmax')
# ])
model = tf.keras.Sequential([
    Conv2D(128, kernel_size=(3, 1), activation='relu', kernel_regularizer=l2(0.02), input_shape=(input_size, 1, 1)),
    MaxPooling2D(pool_size=(2, 1)),
    BatchNormalization(),
    Conv2D(128, kernel_size=(3, 1), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 1)),
    BatchNormalization(),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.2),
    Dense(output_size, activation='softmax')
])

def lr_schedule(epoch):
    return 0.001 * 0.9**epoch

# optimizers 2. adam, 3. sgd , 1. RMSprop, Adagrad
# loss function - 1. sparse_categorical_crossentropy, 2. binary_crossentropy (not working)
optimizer = Adam(learning_rate=0.002)
model.summary()
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Use ImageDataGenerator for data augmentation
history = model.fit(datagen.flow(x_train, y_train, batch_size=10000),
                    steps_per_epoch=len(x_train) / 10000,  # Adjust batch_size
                    epochs=1000,
                    validation_data=(x_test, y_test),
                    #callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]
                    )

# Visualize the result
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
# sns.set_style("white")
# plt.suptitle('Training Results', size=15)

# ax1.plot(epochs, acc, "bo", label="Training acc")
# ax1.plot(epochs, val_acc, "b", label="Validation acc")
# ax1.set_title("Training and validation acc")
# ax1.legend()
# ax1.set_ylabel('Accuracy')  # Set y-axis label
# ax1.set_xlabel('Epoch')

# ax2.plot(epochs, loss, "bo", label="Training loss", color='red')
# ax2.plot(epochs, val_loss, "b", label="Validation loss", color='red')
# ax2.set_title("Training and validation loss")
# ax2.legend()
# ax2.set_ylabel('Cost')  # Set y-axis label
# ax2.set_xlabel('Epoch')

plt.figure(figsize=(10, 5))
sns.set_style("white")

# Plot accuracy
plt.plot(epochs, acc, "bo", label="Training accuracy")
plt.plot(epochs, val_acc, "b", label="Validation accuracy")

plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Value')


plt.show()

test_loss, test_accuracy = model.evaluate(x_test, y_test)
print('\nTest loss: {0:.6f}. Test accuracy: {1:.6f}%'.format(test_loss, test_accuracy * 100.))
